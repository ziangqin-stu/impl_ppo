Compare Parallel/Serial, InvertedPendulum:
    policy_params = ParamDict(
            hidden_dim=64,  # dimension of the hidden state in actor network
            learning_rate=1e-5,  # learning rate of policy update
            discount=0.99,  # discount factor
            lambd=0.95,
            entropy_coef=0.01,  # hyper-parameter to vary the contribution of entropy loss
            critic_coef=0.5,  # Coefficient of critic loss when weighted against actor loss
            clip_param=0.2,
            envs_num=50,
            horizon=200,
            batch_size=64,  # batch size for policy update
            epochs_num=200,  # number of epochs per policy update
        )
        params = ParamDict(
            policy_params=policy_params,
            iter_num=3e3,  # number of training policy iterations
            plotting_iters=300,  # interval for logging graphs and policy rollouts
            seed=123,
            parallel_check_num=100,
            parallel=False,
            env_name='InvertedPendulum-v2',
            save_path='./save',
            prefix='dev_InvertedPendulum_parallel_1'
        )


